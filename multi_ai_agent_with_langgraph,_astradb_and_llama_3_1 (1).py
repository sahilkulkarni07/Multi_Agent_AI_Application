# -*- coding: utf-8 -*-
"""Multi AI Agent with LangGraph, AstraDB and Llama 3.1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XWOwMenJxSbI_KN9xTKYFCjKB7Q5cXcD

# **Multi AI Agent with LangGraph, AstraDB and Llama 3.1**
"""

!pip install langchain cassio langgraph langchain_community langchainhub tiktoken langchain_huggingface langchain-groq

import cassio
from google.colab import userdata
# connection with AstraDB
AstraDB_token = userdata.get('MultiAI_Agent_API')
AstraDB_ID = userdata.get('AstraDB_ID')
cassio.init(token = AstraDB_token,database_id=AstraDB_ID)

# Initiation

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader

# URL info

urls = [
    'https://www.opentext.com/what-is/agentic-ai',
    'https://dev.to/ajmal_hasan/genai-building-rag-systems-with-langchain-4dbp',
    'https://hatchworks.com/blog/ai-agents/ai-agents-explained/'
]

# load the data

docs = [WebBaseLoader(url).load() for url in urls]
doc_list = [item for sublist in docs for item in sublist]
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(encoding_name = 'cl100k_base', chunk_size = 500, chunk_overlap = 0)
docs_split = text_splitter.split_documents(doc_list)

print(docs_split)

pip install -U sentence_transformers huggingface_hub

from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# !huggingface-cli logout

# from sentence_transformers import SentenceTransformer


# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
# embeddings = model.encode()

from langchain.vectorstores import Cassandra
astra_vector_store = Cassandra(embedding = embeddings,
                               table_name = 'Vector_table',
                               session = None,
                               keyspace = None)

from langchain.indexes.vectorstore import VectorStoreIndexWrapper
astra_vector_store.add_documents(docs_split)
print('Inserted %i headlines'%len(docs_split))
astra_vector_index = VectorStoreIndexWrapper(vectorstore = astra_vector_store)

retriever = astra_vector_store.as_retriever()
retriever.invoke('what is the agent ai')

# Langgraph application
from typing import Literal
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel,Field

# Data Model
class RouteQ(BaseModel):
  datasources:Literal['vectorstore','wiki_search'] = Field(
      ...,
      description='Given a user question choose to route it to wikipedia or a vectorstore.'
  )

from langchain_groq import ChatGroq
import os
groq_api_key = userdata.get('groq_API')

llm = ChatGroq(groq_api_key = groq_api_key,
               model_name = 'llama-3.3-70b-versatile'
               )

structured_llm_router = llm.with_structured_output(RouteQ)

system = "You are an expert at routing a user question to a vectorstore or wikipedia. The vectorstore contains document related to agents , prompts engineerings and adversarial attacks. Use the vectorstore for question of these topics.otherwise use wiki-search"
route_prompt = ChatPromptTemplate.from_messages([
    ('system',system),
    ('human','(question)')

])

question_router = route_prompt | structured_llm_router

# question_router = route_prompt | structured_llm_router
print(question_router.invoke({"question": 'who is salman khan'}))
print(question_router.invoke({"question": "What are the types of agentic memory?"}))

!pip install langchain_community
!pip install arxiv wikipedia

from langchain_community.utilities import WikipediaAPIWrapper
from langchain_community.tools import WikipediaQueryRun

api_wrapper = WikipediaAPIWrapper(top_k_results= 1 , doc_content_chars_max=200)
wiki = WikipediaQueryRun(api_wrapper = api_wrapper)

wiki.run('tell me about salman khan')

# AI agent application using langgaraph
from typing import List
from typing_extensions import TypedDict

class GraphState(TypedDict):
  'represent stage of graph'
  question : str
  generation : str
  documents: List[str]

from langchain.schema import Document
def retrieve(state):
  print('retreive')
  questions = state['question']
  documents = retriever.invoke(questions)
  return {'documents':documents,'question': questions}

def wiki_search(state):


    print("---wikipedia---")
    print("---HELLO--")
    question = state["question"]
    print(question)

    # Wiki search
    docs = wiki.invoke({"query": question})
    #print(docs["summary"])
    wiki_results = docs
    wiki_results = Document(page_content=wiki_results)

    return {"documents": wiki_results, "question": question}

### Edges ###


def route_question(state):
    """
    Route question to wiki search or RAG.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """

    print("---ROUTE QUESTION---")
    question = state["question"]
    source = question_router.invoke({"question": question})
    if source.datasources == "wiki_search":
        print("---ROUTE QUESTION TO Wiki SEARCH---")
        return "wiki_search"
    elif source.datasources == "vectorstore":
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"

from langgraph.graph import END, StateGraph, START

workflow = StateGraph(GraphState)
# Define the nodes
workflow.add_node("wiki_search", wiki_search)  # web search
workflow.add_node("retrieve", retrieve)  # retrieve

# Build graph
workflow.add_conditional_edges(
    START,
    route_question,
    {
        "wiki_search": "wiki_search",
        "vectorstore": "retrieve",
    },
)
workflow.add_edge( "retrieve", END)
workflow.add_edge( "wiki_search", END)
# Compile
app = workflow.compile()

from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

from pprint import pprint

# Run
inputs = {
    "question": "What is agent?"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'].page_content)

from pprint import pprint

# Run
inputs = {
    "question": "Avengers"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'])

